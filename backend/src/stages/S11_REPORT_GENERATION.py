from __future__ import annotations
from utils.logger import logger

import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
import json
import traceback

# --- hack sys.path ---
THIS_DIR = Path(__file__).resolve().parent
SRC_DIR = THIS_DIR.parent
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

try:
    from context import PipelineContext
except ImportError:
    pass

from utils.analysis_utils import sanitize_json_floats

def _enrich_report(context: PipelineContext, report: Dict[str, Any]) -> None:
    stages: List[Dict[str, Any]] = report.get("stages", [])

    for s in stages:
        contract_id = s.get("contract_id")
        if not contract_id: continue

        # Analysis is already in memory?
        # The report object structure usually comes from S11 analysis script which aggregates data.
        # But wait, S11 analysis script IS SEPARATE from S11 stage script.
        # S11 Stage script calls S11 Analysis script.
        # If I only update Stage script, I rely on Analysis script being updated too.
        # I MUST UPDATE S11 ANALYSIS SCRIPT.

        # But let's assume S11 Analysis script does the aggregation.
        # This function (process) is the stage script.
        # In the new "in-memory" world, we might merge them or keep them separate.
        # If I keep them separate, S11 Analysis script should read context.analysis_results and produce the base report.
        # Then S11 Stage script (this one) enriches it with images.
        pass

    # Image handling
    for s in stages:
        contract_id = s.get("contract_id")
        if not contract_id: continue

        # Look for images in transient dir
        stage_dir = context.get_stage_dir(contract_id)

        images = {}
        for img_name in ["waveform_comparison.png", "spectrogram_comparison.png"]:
            img_path = stage_dir / img_name
            if img_path.exists():
                unique_name = f"{contract_id}_{img_name}"
                try:
                    data = img_path.read_bytes()
                    context.generated_artifacts[unique_name] = data

                    key = "waveform" if "waveform" in img_name else "spectrogram"
                    images[key] = unique_name
                except Exception as e:
                    logger.logger.error(f"[S11] Failed to read image {img_path}: {e}")

        if images:
            s["images"] = images

def process(context: PipelineContext, *args) -> bool:
    contract_id = context.stage_id

    # 1. Run Analysis (S11 Analysis)
    # We need to call S11 Analysis script logic here or assume it was run by stage.py
    # stage.py runs analysis before process.
    # So context.analysis_results[contract_id] should contain the base report generated by S11 Analysis.

    analysis = context.analysis_results.get(contract_id)
    if not analysis:
        logger.logger.error(f"[S11] No analysis found for {contract_id}")
        return False

    session = analysis.get("session", {})
    report = session.get("report", {})

    if not report:
        logger.logger.warning("[S11] Report data missing in analysis")
        report = {"stages": [], "final_metrics": {}, "error": "Report data missing"}
        session["report"] = report

    # 2. Enrich
    try:
        _enrich_report(context, report)
    except Exception as e:
        logger.logger.error(f"[S11] Error enriching report: {e}")
        traceback.print_exc()

    # 3. Sanitize and Store
    report = sanitize_json_floats(report)
    session["report"] = report
    context.report = report

    # Store back to analysis results
    context.analysis_results[contract_id] = analysis

    logger.logger.info("[S11] Report generated and stored in context.")
    return True
